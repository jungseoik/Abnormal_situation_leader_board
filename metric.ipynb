{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jungseoik/miniconda3/envs/vlm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder preprocessing completed.\n",
      "Categories identified: []\n"
     ]
    }
   ],
   "source": [
    "from pia_bench.bench import PiaBenchMark\n",
    "\n",
    "benchmark_path = \"assets/huggingface_benchmarks_dataset\"\n",
    "pia_benchmark = PiaBenchMark(benchmark_path , \"assets/huggingface_benchmarks_dataset/CFG/topk.json\" )\n",
    "pia_benchmark.preprocess_structure()\n",
    "\n",
    "print(\"Categories identified:\", pia_benchmark.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pia_benchmark.extract_visual_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict, List, Tuple\n",
    "from devmacs_core.devmacs_core import DevMACSCore\n",
    "from devmacs_core.utils.common.cal import scale_sim, loose_similarity\n",
    "from utils.parser import load_config, PromptManager\n",
    "\n",
    "class EventDetector:\n",
    "    def __init__(self, config_path: str):\n",
    "        self.config = load_config(config_path)\n",
    "        self.macs = DevMACSCore()\n",
    "        self.prompt_manager = PromptManager(config_path)\n",
    "        self.sentences = self.prompt_manager.sentences\n",
    "        self.text_vectors = self.macs.get_text_vector(self.sentences)\n",
    "        self.result_pred = None\n",
    "    \n",
    "    def process_video_vectors(self, base_dir: str) -> Dict:\n",
    "        results = {}\n",
    "        \n",
    "        for category in os.listdir(base_dir):\n",
    "            category_path = os.path.join(base_dir, category)\n",
    "            if not os.path.isdir(category_path):\n",
    "                continue\n",
    "                \n",
    "            results[category] = {}\n",
    "            for file in os.listdir(category_path):\n",
    "                if file.endswith('.npy'):\n",
    "                    video_name = os.path.splitext(file)[0]\n",
    "                    file_path = os.path.join(category_path, file)\n",
    "                    results[category][video_name] = self._process_single_vector(file_path)\n",
    "        \n",
    "        self.result_pred = results\n",
    "        return results\n",
    "    \n",
    "    def _process_single_vector(self, vector_path: str) -> Dict:\n",
    "        video_vector = np.load(vector_path)\n",
    "        processed_vectors = []\n",
    "        frame_interval = 15  \n",
    "\n",
    "        for vector in video_vector:\n",
    "            v = vector.squeeze(0)  # (1, 512) -> (512,)\n",
    "            v = torch.from_numpy(v).unsqueeze(0).cuda()  # (512,) -> (1, 512) # GPU로 이동\n",
    "            processed_vectors.append(v)\n",
    "            \n",
    "        frame_results = {}\n",
    "        for vector_idx, v in enumerate(processed_vectors):\n",
    "            actual_frame = vector_idx * frame_interval  # 실제 프레임 번호 계산\n",
    "            sim_scores = loose_similarity(\n",
    "                sequence_output=self.text_vectors.cuda(),  # text vectors도 GPU로\n",
    "                visual_output=v.unsqueeze(1)  # (1, 512) -> (1, 1, 512)\n",
    "            )\n",
    "            frame_results[actual_frame] = self._calculate_alarms(sim_scores)\n",
    "        return frame_results\n",
    "\n",
    "    \n",
    "    def _calculate_alarms(self, sim_scores: torch.Tensor) -> Dict:\n",
    "        \"\"\"\n",
    "        유사도 점수를 기반으로 각 이벤트의 알람 상태 계산\n",
    "        Returns:\n",
    "            Dict: {\n",
    "                '이벤트명': {\n",
    "                    'alarm': 0 또는 1,  # 0: Normal, 1: Alarm\n",
    "                    'scores': [스코어들],\n",
    "                    'top_k_types': [타입들]\n",
    "                }\n",
    "            }\n",
    "        \"\"\"\n",
    "        event_alarms = {}\n",
    "        \n",
    "        for event_config in self.config['PROMPT_CFG']:\n",
    "            event = event_config['event']\n",
    "            top_k = event_config['top_candidates']\n",
    "            threshold = event_config['alert_threshold']\n",
    "            \n",
    "            event_prompts = self._get_event_prompts(event)\n",
    "            event_scores = sim_scores[event_prompts['indices']]\n",
    "            \n",
    "            top_k_values, top_k_indices = torch.topk(event_scores, min(top_k, len(event_scores)))\n",
    "            \n",
    "            abnormal_count = sum(1 for idx in top_k_indices \n",
    "                            if event_prompts['types'][idx.item()] == 'abnormal')\n",
    "            \n",
    "            event_alarms[event] = {\n",
    "                'alarm': 1 if abnormal_count >= threshold else 0,  # 0: Normal, 1: Alarm\n",
    "                'scores': top_k_values.tolist(),\n",
    "                'top_k_types': [event_prompts['types'][idx.item()] for idx in top_k_indices]\n",
    "            }\n",
    "            \n",
    "        return event_alarms\n",
    "    \n",
    "    def _get_event_prompts(self, event: str) -> Dict:\n",
    "        \"\"\"\n",
    "        특정 이벤트의 모든 프롬프트 정보 반환\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        types = []\n",
    "        \n",
    "        for event_idx, event_config in enumerate(self.config['PROMPT_CFG']):\n",
    "            if event_config['event'] == event:\n",
    "                for status in ['normal', 'abnormal']:\n",
    "                    for prompt_idx in range(len(event_config['prompts'][status])):\n",
    "                        indices.append(len(indices))  # 실제 인덱스로 변환 필요\n",
    "                        types.append(status)\n",
    "                        \n",
    "        return {'indices': indices, 'types': types}\n",
    "\n",
    "detector = EventDetector('assets/huggingface_benchmarks_dataset/CFG/topk.json')\n",
    "results = detector.process_video_vectors(pia_benchmark.vector_video_path)\n",
    "\n",
    "for category, videos in results.items():\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    for video_name, frames in videos.items():\n",
    "        print(f\"\\nVideo: {video_name}\")\n",
    "        for frame_idx, alarms in frames.items():\n",
    "            print(f\"\\nFrame {frame_idx}:\")\n",
    "            for event, status in alarms.items():\n",
    "                print(f\"{event}: {status['alarm']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jungseoik/data/Abnormal_situation_leader_board/DevMACS-AI-solution-devmacs/Package-Common-AI-pia_ai_package/packages/pia/ai/tasks/T2VRet/models/clip4clip/main.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(self.config.model_path, map_location=self.config.device)\n",
      "Processing videos:   0%|          | 0/94 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict, List, Tuple\n",
    "from devmacs_core.devmacs_core import DevMACSCore\n",
    "from devmacs_core.utils.common.cal import scale_sim, loose_similarity\n",
    "from utils.parser import load_config, PromptManager\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "class EventDetector:\n",
    "    def __init__(self, config_path: str):\n",
    "        self.config = load_config(config_path)\n",
    "        self.macs = DevMACSCore()\n",
    "        self.prompt_manager = PromptManager(config_path)\n",
    "        self.sentences = self.prompt_manager.sentences\n",
    "        self.text_vectors = self.macs.get_text_vector(self.sentences)\n",
    "        \n",
    "    def process_and_save_predictions(self, vector_base_dir: str, label_base_dir: str, save_base_dir: str):\n",
    "        \"\"\"비디오 벡터를 처리하고 결과를 CSV로 저장\"\"\"\n",
    "        \n",
    "\n",
    "            # 전체 비디오 파일 수 계산\n",
    "        total_videos = sum(len([f for f in os.listdir(os.path.join(vector_base_dir, d)) \n",
    "                                if f.endswith('.npy')]) \n",
    "                            for d in os.listdir(vector_base_dir) \n",
    "                            if os.path.isdir(os.path.join(vector_base_dir, d)))\n",
    "        pbar = tqdm(total=total_videos, desc=\"Processing videos\")\n",
    "        \n",
    "        for category in os.listdir(vector_base_dir):\n",
    "            category_path = os.path.join(vector_base_dir, category)\n",
    "            if not os.path.isdir(category_path):\n",
    "                continue\n",
    "            \n",
    "            # 저장 디렉토리 생성\n",
    "            save_category_dir = os.path.join(save_base_dir, category)\n",
    "            os.makedirs(save_category_dir, exist_ok=True)\n",
    "            \n",
    "            for file in os.listdir(category_path):\n",
    "                if file.endswith('.npy'):\n",
    "                    video_name = os.path.splitext(file)[0]\n",
    "                    vector_path = os.path.join(category_path, file)\n",
    "                    \n",
    "                    # 라벨 파일 읽기\n",
    "                    label_path = os.path.join(label_base_dir, category, f\"{video_name}.json\")\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        label_data = json.load(f)\n",
    "                        total_frames = label_data['video_info']['total_frame']\n",
    "                    \n",
    "                    # 예측 결과 생성 및 저장\n",
    "                    self._process_and_save_single_video(\n",
    "                        vector_path=vector_path,\n",
    "                        total_frames=total_frames,\n",
    "                        save_path=os.path.join(save_category_dir, f\"{video_name}.csv\")\n",
    "                    )\n",
    "                    pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "    def _process_and_save_single_video(self, vector_path: str, total_frames: int, save_path: str):\n",
    "        \"\"\"단일 비디오 처리 및 저장\"\"\"\n",
    "        # 기본 예측 수행\n",
    "        sparse_predictions = self._process_single_vector(vector_path)\n",
    "        \n",
    "        # 데이터프레임으로 변환 및 확장\n",
    "        df = self._expand_predictions(sparse_predictions, total_frames)\n",
    "        \n",
    "        # CSV로 저장\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "    def _process_single_vector(self, vector_path: str) -> Dict:\n",
    "        \"\"\"기존 예측 로직\"\"\"\n",
    "        video_vector = np.load(vector_path)\n",
    "        processed_vectors = []\n",
    "        frame_interval = 15\n",
    "        \n",
    "        for vector in video_vector:\n",
    "            v = vector.squeeze(0)  # numpy array\n",
    "            v = torch.from_numpy(v).unsqueeze(0).cuda()  # torch tensor로 변환 후 GPU로\n",
    "            processed_vectors.append(v)\n",
    "        \n",
    "        frame_results = {}\n",
    "        for vector_idx, v in enumerate(processed_vectors):\n",
    "            actual_frame = vector_idx * frame_interval\n",
    "            sim_scores = loose_similarity(\n",
    "                sequence_output=self.text_vectors.cuda(),\n",
    "                visual_output=v.unsqueeze(1)\n",
    "            )\n",
    "            frame_results[actual_frame] = self._calculate_alarms(sim_scores)\n",
    "            \n",
    "        return frame_results\n",
    "\n",
    "    def _expand_predictions(self, sparse_predictions: Dict, total_frames: int) -> pd.DataFrame:\n",
    "        \"\"\"예측을 전체 프레임으로 확장\"\"\"\n",
    "        # 카테고리 목록 추출 (첫 번째 프레임의 알람 결과에서)\n",
    "        first_frame = list(sparse_predictions.keys())[0]\n",
    "        categories = list(sparse_predictions[first_frame].keys())\n",
    "        \n",
    "        # 전체 프레임 생성\n",
    "        df = pd.DataFrame({'frame': range(total_frames)})\n",
    "        \n",
    "        # 각 카테고리에 대한 알람 값 초기화\n",
    "        for category in categories:\n",
    "            df[category] = 0\n",
    "        \n",
    "        # 예측값 채우기\n",
    "        frame_keys = sorted(sparse_predictions.keys())\n",
    "        for i in range(len(frame_keys)):\n",
    "            current_frame = frame_keys[i]\n",
    "            next_frame = frame_keys[i + 1] if i + 1 < len(frame_keys) else total_frames\n",
    "            \n",
    "            # 각 카테고리의 알람 값 설정\n",
    "            for category in categories:\n",
    "                alarm_value = sparse_predictions[current_frame][category]['alarm']\n",
    "                df.loc[current_frame:next_frame-1, category] = alarm_value\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def _calculate_alarms(self, sim_scores: torch.Tensor) -> Dict:\n",
    "        \"\"\"유사도 점수를 기반으로 각 이벤트의 알람 상태 계산\"\"\"\n",
    "        # 로거 설정\n",
    "        log_filename = f\"alarm_calculation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "        logging.basicConfig(\n",
    "            filename=log_filename,\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        logger = logging.getLogger(__name__)\n",
    "        \n",
    "        event_alarms = {}\n",
    "        \n",
    "        for event_config in self.config['PROMPT_CFG']:\n",
    "            event = event_config['event']\n",
    "            top_k = event_config['top_candidates']\n",
    "            threshold = event_config['alert_threshold']\n",
    "            \n",
    "            logger.info(f\"\\nProcessing event: {event}\")\n",
    "            logger.info(f\"Top K: {top_k}, Threshold: {threshold}\")\n",
    "            \n",
    "            event_prompts = self._get_event_prompts(event)\n",
    "\n",
    "            # logger.debug(f\"\\nEvent Prompts Debug for {event}:\")\n",
    "            # logger.debug(f\"Indices: {event_prompts['indices']}\")\n",
    "            # logger.debug(f\"Types: {event_prompts['types']}\")\n",
    "            # logger.debug(f\"\\nSim Scores Debug:\")\n",
    "            # logger.debug(f\"Shape: {sim_scores.shape}\")\n",
    "            # logger.debug(f\"Raw scores: {sim_scores}\")\n",
    "            event_scores = sim_scores[event_prompts['indices']]\n",
    "            # logger.debug(f\"Event scores shape: {event_scores.shape}\")\n",
    "            # logger.debug(f\"Event scores: {event_scores}\")\n",
    "            # 각 프롬프트와 점수 출력\n",
    "            logger.info(\"\\nAll prompts and scores:\")\n",
    "            for idx, (score, prompt_type) in enumerate(zip(event_scores, event_prompts['types'])):\n",
    "                logger.info(f\"Type: {prompt_type}, Score: {score.item():.4f}\")\n",
    "            \n",
    "            top_k_values, top_k_indices = torch.topk(event_scores, min(top_k, len(event_scores)))\n",
    "            \n",
    "            # Top K 결과 출력\n",
    "            logger.info(f\"\\nTop {top_k} selections:\")\n",
    "            for idx, (value, index) in enumerate(zip(top_k_values, top_k_indices)):\n",
    "                # indices[index]가 아닌 index를 직접 사용\n",
    "                prompt_type = event_prompts['types'][index]  # 수정된 부분\n",
    "                logger.info(f\"Rank {idx+1}: Type: {prompt_type}, Score: {value.item():.4f}\")\n",
    "\n",
    "            abnormal_count = sum(1 for idx in top_k_indices \n",
    "                    if event_prompts['types'][idx] == 'abnormal')  # 수정된 부분\n",
    "            # for idx, (value, orig_idx) in enumerate(zip(top_k_values, top_k_indices)):\n",
    "            #     prompt_type = event_prompts['types'][orig_idx.item()]\n",
    "            #     logger.info(f\"Rank {idx+1}: Type: {prompt_type}, Score: {value.item():.4f}\")\n",
    "            \n",
    "            # abnormal_count = sum(1 for idx in top_k_indices \n",
    "            #                 if event_prompts['types'][idx.item()] == 'abnormal')\n",
    "            \n",
    "            # 알람 결정 과정 출력\n",
    "            logger.info(f\"\\nAbnormal count: {abnormal_count}\")\n",
    "            alarm_result = 1 if abnormal_count >= threshold else 0\n",
    "            logger.info(f\"Final alarm decision: {alarm_result}\")\n",
    "            logger.info(\"-\" * 50)\n",
    "            \n",
    "            event_alarms[event] = {\n",
    "                'alarm': alarm_result,\n",
    "                'scores': top_k_values.tolist(),\n",
    "                'top_k_types': [event_prompts['types'][idx.item()] for idx in top_k_indices]\n",
    "            }\n",
    "        \n",
    "        # 로거 종료\n",
    "        logging.shutdown()\n",
    "                \n",
    "        return event_alarms\n",
    "    \n",
    "    def _get_event_prompts(self, event: str) -> Dict:\n",
    "        \"\"\"\n",
    "        특정 이벤트의 모든 프롬프트 정보 반환\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        types = []\n",
    "        \n",
    "        for event_idx, event_config in enumerate(self.config['PROMPT_CFG']):\n",
    "            if event_config['event'] == event:\n",
    "                for status in ['normal', 'abnormal']:\n",
    "                    for prompt_idx in range(len(event_config['prompts'][status])):\n",
    "                        indices.append(len(indices))  # 실제 인덱스로 변환 필요\n",
    "                        types.append(status)\n",
    "                        \n",
    "        return {'indices': indices, 'types': types}\n",
    "    \n",
    "detector = EventDetector('assets/huggingface_benchmarks_dataset/CFG/topk.json')\n",
    "detector.process_and_save_predictions(\n",
    "    vector_base_dir='assets/huggingface_benchmarks_dataset/vector/video',\n",
    "    label_base_dir='assets/huggingface_benchmarks_dataset/dataset',\n",
    "    save_base_dir='assets/huggingface_benchmarks_dataset/alram'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "class AlarmExpander:\n",
    "    def __init__(self, category_results: Dict, video_metadata_path: str):\n",
    "        \"\"\"\n",
    "        category_results: Detector의 결과로 나온 알람 값\n",
    "        video_metadata_path: 라벨링 파일이 저장된 경로\n",
    "        \"\"\"\n",
    "        self.category_results = category_results\n",
    "        self.video_metadata_path = video_metadata_path\n",
    "\n",
    "    def expand_alarms(self) -> Dict:\n",
    "        \"\"\"\n",
    "        비디오의 알람 값을 전체 프레임에 대해 확장.\n",
    "\n",
    "        \"\"\"\n",
    "        expanded_results = {}\n",
    "\n",
    "        for category, videos in self.category_results.items():\n",
    "            expanded_results[category] = {}\n",
    "            for video_name, alarms in videos.items():\n",
    "                expanded_alarms = {}\n",
    "                \n",
    "                # 라벨링 파일에서 비디오 총 프레임 수 읽기\n",
    "                label_path = f\"{self.video_metadata_path}/{category}/{video_name}.json\"\n",
    "                with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    video_metadata = json.load(f)\n",
    "                    total_frames = video_metadata[\"video_info\"][\"total_frame\"]\n",
    "                \n",
    "                # 프레임 간격 확장\n",
    "                frame_keys = sorted(alarms.keys())\n",
    "                for i in range(len(frame_keys)):\n",
    "                    current_frame = frame_keys[i]\n",
    "                    if i + 1 < len(frame_keys):\n",
    "                        next_frame = frame_keys[i + 1]\n",
    "                    else:\n",
    "                        next_frame = total_frames\n",
    "\n",
    "                    for frame in range(current_frame, next_frame):\n",
    "                        expanded_alarms[frame] = alarms[current_frame]\n",
    "                \n",
    "                expanded_results[category][video_name] = expanded_alarms\n",
    "        \n",
    "        return expanded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detector 결과와 라벨링 파일 경로 설정\n",
    "detector_results = {\n",
    "    \"F\": {\n",
    "        \"cat copy 2\": {\n",
    "            0: {\"D\": 0, \"S\": 0, \"V\": 0, \"F\": 0},\n",
    "            15: {\"D\": 0, \"S\": 0, \"V\": 0, \"F\": 0},\n",
    "            30: {\"D\": 0, \"S\": 0, \"V\": 0, \"F\": 0},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "label_path = \"assets/huggingface_benchmarks_dataset/dataset\"\n",
    "# expander = AlarmExpander(detector_results, label_path)\n",
    "expander = AlarmExpander(results, label_path)\n",
    "\n",
    "expanded_results = expander.expand_alarms()\n",
    "print(expanded_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class MetricsEvaluator:\n",
    "    def __init__(self, expanded_results: Dict, label_dir: str):\n",
    "        self.expanded_results = expanded_results\n",
    "        self.label_dir = label_dir\n",
    "        self.metrics = {}\n",
    "\n",
    "    def load_label_file(self, category: str, video_name: str) -> Dict:\n",
    "        \"\"\"라벨링 파일 로드\"\"\"\n",
    "        label_path = os.path.join(self.label_dir, category, f\"{video_name}.json\")\n",
    "        with open(label_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def create_frame_level_ground_truth(self, label_data: Dict, category: str, total_frames: int) -> np.ndarray:\n",
    "        \"\"\"타임스탬프(프레임) 기반으로 ground truth 생성\"\"\"\n",
    "        ground_truth = np.zeros(total_frames)\n",
    "        \n",
    "        for clip in label_data['clips']:\n",
    "            clip_data = list(clip.values())[0]\n",
    "            if clip_data['category'] == category:\n",
    "                start_frame = clip_data['timestamp'][0]  # 직접 프레임 인덱스\n",
    "                end_frame = clip_data['timestamp'][1]    # 직접 프레임 인덱스\n",
    "                ground_truth[start_frame:end_frame+1] = 1\n",
    "        \n",
    "        return ground_truth\n",
    "\n",
    "    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict:\n",
    "        \"\"\"성능 지표 계산\"\"\"\n",
    "        # 특이도 계산을 위한 TN, FP\n",
    "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        \n",
    "        metrics = {\n",
    "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    def evaluate(self) -> Dict:\n",
    "        \"\"\"전체 평가 수행\"\"\"\n",
    "        for category, videos in self.expanded_results.items():\n",
    "            self.metrics[category] = {}\n",
    "            \n",
    "            for video_name, frame_results in videos.items():\n",
    "                try:\n",
    "                    # 라벨 파일 로드\n",
    "                    label_data = self.load_label_file(category, video_name)\n",
    "                    total_frames = label_data['video_info']['total_frame']\n",
    "                    \n",
    "                    # Ground truth 생성\n",
    "                    ground_truth = self.create_frame_level_ground_truth(label_data, category, total_frames)\n",
    "                    \n",
    "                    # 예측값 배열 생성 - 수정된 부분\n",
    "                    predictions = np.zeros(total_frames)\n",
    "                    for frame_idx, alarms in frame_results.items():\n",
    "                        # alarms[category]['alarm']에서 알람 값 가져오기\n",
    "                        predictions[int(frame_idx)] = alarms[category]['alarm']\n",
    "                    \n",
    "                    # 디버깅을 위한 출력\n",
    "                    print(f\"\\nProcessing {video_name} in category {category}\")\n",
    "                    print(f\"Total frames: {total_frames}\")\n",
    "                    print(f\"Ground truth sum: {np.sum(ground_truth)}\")\n",
    "                    print(f\"Predictions sum: {np.sum(predictions)}\")\n",
    "                    # print(f\"Ground truth samples: {ground_truth[:10]}\")\n",
    "                    # print(f\"Predictions samples: {predictions[:10]}\")\n",
    "                    print(f\"Ground truth values: {np.where(ground_truth == 1)[0]}\")  # 1인 프레임의 인덱스\n",
    "                    print(f\"Prediction values: {np.where(predictions == 1)[0]}\")     # 1인 프레임의 인덱스\n",
    "                    print(f\"All prediction values: {predictions}\")  # 전체 예측값\n",
    "                    # 메트릭 계산\n",
    "                    video_metrics = self.calculate_metrics(ground_truth, predictions)\n",
    "                    self.metrics[category][video_name] = video_metrics\n",
    "                    \n",
    "                    print(f\"Metrics for {video_name}: {video_metrics}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {video_name} in category {category}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "        return self.metrics\n",
    "\n",
    "# 사용 예시:\n",
    "def evaluate_results(expanded_results: Dict, label_dir: str) -> Dict:\n",
    "    evaluator = MetricsEvaluator(expanded_results, label_dir)\n",
    "    return evaluator.evaluate()\n",
    "\n",
    "# 평균 성능 계산을 위한 함수\n",
    "def calculate_average_metrics(metrics: Dict) -> Dict:\n",
    "    \"\"\"각 카테고리별 평균 성능 계산\"\"\"\n",
    "    avg_metrics = {}\n",
    "    \n",
    "    for category, video_metrics in metrics.items():\n",
    "        avg_metrics[category] = {\n",
    "            metric: np.mean([vm[metric] for vm in video_metrics.values() if metric in vm])\n",
    "            for metric in ['f1', 'precision', 'recall', 'accuracy', 'specificity']\n",
    "        }\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "# 실행\n",
    "label_dir = \"assets/PIA_FSV copy/dataset\"\n",
    "metrics = evaluate_results(expanded_results, label_dir)\n",
    "avg_metrics = calculate_average_metrics(metrics)\n",
    "\n",
    "# 결과 출력\n",
    "for category, category_metrics in metrics.items():\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    for video_name, video_metrics in category_metrics.items():\n",
    "        print(f\"\\n{video_name}:\")\n",
    "        for metric_name, value in video_metrics.items():\n",
    "            print(f\"{metric_name}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nAverage Metrics:\")\n",
    "for category, category_avg in avg_metrics.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for metric_name, value in category_avg.items():\n",
    "        print(f\"{metric_name}: {value:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
